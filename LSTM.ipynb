{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nding-Capstone/BLSTM/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VHXhE0Nat6M",
        "colab_type": "code",
        "outputId": "f26e66bb-1408-44aa-c911-8f706eaa0192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "!pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.2.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torch-1.2.0%2Bcu92-cp36-cp36m-manylinux1_x86_64.whl (663.1MB)\n",
            "\u001b[K     |████████████████████████████████| 663.1MB 27kB/s \n",
            "\u001b[?25hCollecting torchvision==0.4.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.4.0%2Bcu92-cp36-cp36m-manylinux1_x86_64.whl (8.8MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8MB 31.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.2.0+cu92) (1.18.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0+cu92) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0+cu92) (1.12.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Successfully installed torch-1.2.0+cu92 torchvision-0.4.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm_OnXYJJxWY",
        "colab_type": "code",
        "outputId": "fc1c634a-f46d-405d-c9af-8b7daf320302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnEKR4h6Ub55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7MRe77eL-KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNJE97KziqrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_total_class = np.load('/content/drive/My Drive/Colab Notebooks/캡스톤_동영상/캡스톤_동영상/npy/train_total.npy')\n",
        "test_total_class = np.load('/content/drive/My Drive/Colab Notebooks/캡스톤_동영상/캡스톤_동영상/npy/test_total.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVw3FXC3jGjw",
        "colab_type": "code",
        "outputId": "5d793b2f-3c60-4615-bb5e-aef0414214e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(train_total_class.shape)\n",
        "print(test_total_class.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 40, 8)\n",
            "(150, 20, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Op1lUDWR_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_cnt = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QECLwp_ZaQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = torch.from_numpy(train_total_class[:,:class_cnt*2,:])\n",
        "x_test_data = torch.from_numpy(test_total_class[:,:class_cnt,:])\n",
        "tmp_train = list()\n",
        "tmp_test = list()\n",
        "for i in range(class_cnt) :  \n",
        "  tmp_train.append(i)\n",
        "  tmp_train.append(i)\n",
        "  tmp_test.append(i)\n",
        "y_test_data = np.array(tmp_test)\n",
        "y_test_data = torch.from_numpy(y_test_data)\n",
        "y_data = np.array(tmp_train)\n",
        "y_data = torch.from_numpy(y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sea9rvCcOSZ",
        "colab_type": "code",
        "outputId": "3234362c-46d8-4ab6-97e7-f3a01ddd74f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(x_data.size())\n",
        "print(y_data.size())\n",
        "print(x_test_data.size())\n",
        "print(y_test_data.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([150, 40, 8])\n",
            "torch.Size([40])\n",
            "torch.Size([150, 20, 8])\n",
            "torch.Size([20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S274QGEnVzlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        " \n",
        "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
        "                    num_layers=2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        " \n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True)\n",
        " \n",
        "        # Define the output layer\n",
        "        self.linear = nn.Linear(self.hidden_dim*2, output_dim)\n",
        " \n",
        "    def init_hidden(self):\n",
        "        # This is what we'll initialise our hidden state as\n",
        "        return (torch.zeros(self.num_layers*2, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers*2, self.batch_size, self.hidden_dim))\n",
        " \n",
        "    def forward(self, input):\n",
        "        # Forward pass through LSTM layer\n",
        "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
        "        # shape of self.hidden: (a, b), where a and b both \n",
        "        # have shape (num_layers, batch_size, hidden_dim).\n",
        "        input = input.float()\n",
        "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
        "        feature_map = (lstm_out[-1].view(self.batch_size, -1))\n",
        "        feature_map = feature_map.view(feature_map.size(0),feature_map.size(1),1,1)\n",
        "        # Only take the output from the final timetep\n",
        "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
        "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
        "\n",
        "        return y_pred,feature_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3P4olxzbiR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_batch_size = 1\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "base_model = LSTM(8, 32, batch_size=num_batch_size, output_dim=class_cnt, num_layers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZlDaIbobTgk",
        "colab_type": "code",
        "outputId": "6d17f66f-6fba-4dc8-ef98-3cb5dddf4024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "\n",
        "dataset = TensorDataset(x_data.permute(1,0,2), y_data.type(torch.LongTensor))\n",
        "loader = DataLoader(dataset, batch_size=num_batch_size, shuffle=True)\n",
        "\n",
        "dataset_test = TensorDataset(x_test_data.permute(1,0,2), y_test_data.type(torch.LongTensor))\n",
        "loader_test = DataLoader(dataset_test, batch_size=num_batch_size, shuffle=True)\n",
        "\n",
        "optimiser = torch.optim.Adam(base_model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "hist = np.zeros(100)\n",
        "\n",
        "#####################\n",
        "# Train model\n",
        "#####################\n",
        "  \n",
        "best_acc_test = 0\n",
        "best_acc_train = 0\n",
        "\n",
        "for t in range(100):\n",
        "  \n",
        "    batch_loss = 0.0\n",
        "    acc_val = 0.0\n",
        "    acc_val_train = 0.0\n",
        "    sum_test = 0\n",
        "    sum_train = 0\n",
        "    correct = 0\n",
        "    correct_train = 0\n",
        "\n",
        "    for xx, yy in loader:\n",
        "      sum_train += 1\n",
        "      # Initialise hidden state\n",
        "      # Don't do this if you want your LSTM to be stateful\n",
        "      base_model.hidden = base_model.init_hidden()\n",
        "\n",
        "      xx = xx.permute(1,0,2)\n",
        "      # Clear stored gradient\n",
        "      base_model.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      y_pred, _ = base_model(xx)\n",
        "      train_label = torch.argmax(y_pred)\n",
        "      if train_label.item() == yy.item() : \n",
        "        correct_train += 1\n",
        "      loss = loss_fn(y_pred, yy)\n",
        "      batch_loss += loss.item()\n",
        "\n",
        "      # Zero out gradient, else they will accumulate between epochs\n",
        "      optimiser.zero_grad()\n",
        "\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Update parameters\n",
        "      optimiser.step()\n",
        "    \n",
        "    for xx_test, yy_test in loader_test:\n",
        "      sum_test += 1\n",
        "      xx_test = xx_test.permute(1,0,2)\n",
        "      y_test_pred, _ = base_model(xx_test)\n",
        "      _,test_label = torch.topk(y_test_pred,3)\n",
        "      if yy_test.item() in test_label : \n",
        "        correct += 1\n",
        "    \n",
        "    \n",
        "    acc_val_train = correct_train/sum_train\n",
        "    acc_val = correct/sum_test\n",
        "\n",
        "    print(\"Epoch : \", t ,\"Loss : \", batch_loss, \"Train_ACC : \" , acc_val_train , \"Test_ACC : \" , acc_val)\n",
        "    if best_acc_test <= acc_val and best_acc_train <= acc_val_train:\n",
        "      best_acc_test = acc_val\n",
        "      best_acc_train = acc_val_train\n",
        "      torch.save(base_model.state_dict(),'/content/drive/My Drive/Colab Notebooks/chekcpoint_20200506_best_top3.pth')\n",
        "      print(\"save_check_point\")\n",
        "\n",
        "\n",
        "print(\"Best_top3_check_point - Train_ACC : \", best_acc_train , \" Test_ACC : \", best_acc_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  0 Loss :  121.73178839683533 Train_ACC :  0.025 Test_ACC :  0.15\n",
            "save_check_point\n",
            "Epoch :  1 Loss :  123.0019690990448 Train_ACC :  0.0 Test_ACC :  0.2\n",
            "Epoch :  2 Loss :  120.89031386375427 Train_ACC :  0.05 Test_ACC :  0.2\n",
            "save_check_point\n",
            "Epoch :  3 Loss :  119.83864688873291 Train_ACC :  0.05 Test_ACC :  0.25\n",
            "save_check_point\n",
            "Epoch :  4 Loss :  117.54682850837708 Train_ACC :  0.075 Test_ACC :  0.3\n",
            "save_check_point\n",
            "Epoch :  5 Loss :  117.67785322666168 Train_ACC :  0.05 Test_ACC :  0.3\n",
            "Epoch :  6 Loss :  112.58218804001808 Train_ACC :  0.1 Test_ACC :  0.15\n",
            "Epoch :  7 Loss :  109.82833695411682 Train_ACC :  0.125 Test_ACC :  0.3\n",
            "save_check_point\n",
            "Epoch :  8 Loss :  111.43946471810341 Train_ACC :  0.15 Test_ACC :  0.25\n",
            "Epoch :  9 Loss :  104.83003604412079 Train_ACC :  0.175 Test_ACC :  0.25\n",
            "Epoch :  10 Loss :  101.36404103040695 Train_ACC :  0.275 Test_ACC :  0.25\n",
            "Epoch :  11 Loss :  100.84474366903305 Train_ACC :  0.225 Test_ACC :  0.3\n",
            "save_check_point\n",
            "Epoch :  12 Loss :  92.10177966952324 Train_ACC :  0.2 Test_ACC :  0.25\n",
            "Epoch :  13 Loss :  87.9667628929019 Train_ACC :  0.325 Test_ACC :  0.25\n",
            "Epoch :  14 Loss :  82.02171565592289 Train_ACC :  0.275 Test_ACC :  0.45\n",
            "save_check_point\n",
            "Epoch :  15 Loss :  76.1336969062686 Train_ACC :  0.375 Test_ACC :  0.2\n",
            "Epoch :  16 Loss :  77.1069744117558 Train_ACC :  0.25 Test_ACC :  0.35\n",
            "Epoch :  17 Loss :  69.90311239659786 Train_ACC :  0.25 Test_ACC :  0.35\n",
            "Epoch :  18 Loss :  66.37895089946687 Train_ACC :  0.3 Test_ACC :  0.4\n",
            "Epoch :  19 Loss :  61.99021497834474 Train_ACC :  0.4 Test_ACC :  0.25\n",
            "Epoch :  20 Loss :  61.1786327958107 Train_ACC :  0.35 Test_ACC :  0.35\n",
            "Epoch :  21 Loss :  48.58523967303336 Train_ACC :  0.525 Test_ACC :  0.3\n",
            "Epoch :  22 Loss :  53.60216353647411 Train_ACC :  0.45 Test_ACC :  0.45\n",
            "save_check_point\n",
            "Epoch :  23 Loss :  53.84940950572491 Train_ACC :  0.525 Test_ACC :  0.25\n",
            "Epoch :  24 Loss :  50.984363593161106 Train_ACC :  0.55 Test_ACC :  0.35\n",
            "Epoch :  25 Loss :  41.32824106235057 Train_ACC :  0.575 Test_ACC :  0.3\n",
            "Epoch :  26 Loss :  34.33513932209462 Train_ACC :  0.625 Test_ACC :  0.4\n",
            "Epoch :  27 Loss :  30.499714640900493 Train_ACC :  0.75 Test_ACC :  0.35\n",
            "Epoch :  28 Loss :  29.56981529854238 Train_ACC :  0.7 Test_ACC :  0.4\n",
            "Epoch :  29 Loss :  22.040637906640768 Train_ACC :  0.775 Test_ACC :  0.3\n",
            "Epoch :  30 Loss :  18.66192358569242 Train_ACC :  0.825 Test_ACC :  0.35\n",
            "Epoch :  31 Loss :  21.1068285536021 Train_ACC :  0.75 Test_ACC :  0.35\n",
            "Epoch :  32 Loss :  18.70522697782144 Train_ACC :  0.825 Test_ACC :  0.2\n",
            "Epoch :  33 Loss :  23.31116090575233 Train_ACC :  0.8 Test_ACC :  0.35\n",
            "Epoch :  34 Loss :  32.790996262803674 Train_ACC :  0.75 Test_ACC :  0.4\n",
            "Epoch :  35 Loss :  14.168438049033284 Train_ACC :  0.875 Test_ACC :  0.35\n",
            "Epoch :  36 Loss :  26.53493318898836 Train_ACC :  0.775 Test_ACC :  0.3\n",
            "Epoch :  37 Loss :  16.72801530919969 Train_ACC :  0.8 Test_ACC :  0.35\n",
            "Epoch :  38 Loss :  9.644801841350272 Train_ACC :  0.95 Test_ACC :  0.35\n",
            "Epoch :  39 Loss :  5.752746740356088 Train_ACC :  0.975 Test_ACC :  0.4\n",
            "Epoch :  40 Loss :  4.423381466884166 Train_ACC :  0.95 Test_ACC :  0.45\n",
            "save_check_point\n",
            "Epoch :  41 Loss :  4.55519936606288 Train_ACC :  0.975 Test_ACC :  0.35\n",
            "Epoch :  42 Loss :  4.539210835006088 Train_ACC :  0.95 Test_ACC :  0.4\n",
            "Epoch :  43 Loss :  6.404396514641121 Train_ACC :  0.925 Test_ACC :  0.45\n",
            "Epoch :  44 Loss :  3.8307166651356965 Train_ACC :  0.975 Test_ACC :  0.35\n",
            "Epoch :  45 Loss :  3.0789125775918365 Train_ACC :  0.975 Test_ACC :  0.4\n",
            "Epoch :  46 Loss :  11.10666749230586 Train_ACC :  0.875 Test_ACC :  0.35\n",
            "Epoch :  47 Loss :  18.604248406598344 Train_ACC :  0.875 Test_ACC :  0.4\n",
            "Epoch :  48 Loss :  21.887118403799832 Train_ACC :  0.825 Test_ACC :  0.3\n",
            "Epoch :  49 Loss :  14.536672886577435 Train_ACC :  0.875 Test_ACC :  0.35\n",
            "Epoch :  50 Loss :  20.904569345642813 Train_ACC :  0.8 Test_ACC :  0.4\n",
            "Epoch :  51 Loss :  10.916608064784668 Train_ACC :  0.925 Test_ACC :  0.35\n",
            "Epoch :  52 Loss :  11.828196057293098 Train_ACC :  0.9 Test_ACC :  0.35\n",
            "Epoch :  53 Loss :  13.782068941975012 Train_ACC :  0.925 Test_ACC :  0.3\n",
            "Epoch :  54 Loss :  4.565932192374021 Train_ACC :  0.975 Test_ACC :  0.35\n",
            "Epoch :  55 Loss :  3.049208107870072 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  56 Loss :  2.9515000790706836 Train_ACC :  0.975 Test_ACC :  0.35\n",
            "Epoch :  57 Loss :  1.6217839147429913 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  58 Loss :  1.3389824758050963 Train_ACC :  1.0 Test_ACC :  0.3\n",
            "Epoch :  59 Loss :  1.4997054196428508 Train_ACC :  0.975 Test_ACC :  0.25\n",
            "Epoch :  60 Loss :  2.3740306126419455 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  61 Loss :  3.1992108380654827 Train_ACC :  0.975 Test_ACC :  0.25\n",
            "Epoch :  62 Loss :  2.0388800121145323 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  63 Loss :  1.434299024171196 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  64 Loss :  1.9260148096363991 Train_ACC :  0.975 Test_ACC :  0.35\n",
            "Epoch :  65 Loss :  2.3425745464046486 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  66 Loss :  2.0345158787677065 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  67 Loss :  1.302708060538862 Train_ACC :  0.975 Test_ACC :  0.35\n",
            "Epoch :  68 Loss :  1.471265685860999 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  69 Loss :  1.9075608997954987 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  70 Loss :  2.0265753753483295 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  71 Loss :  1.955220150121022 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  72 Loss :  2.259468165342696 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  73 Loss :  1.2461726375040598 Train_ACC :  1.0 Test_ACC :  0.3\n",
            "Epoch :  74 Loss :  1.7784786339034326 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  75 Loss :  1.5585756400832906 Train_ACC :  0.975 Test_ACC :  0.35\n",
            "Epoch :  76 Loss :  1.990970691316761 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  77 Loss :  1.6468402641476132 Train_ACC :  0.975 Test_ACC :  0.35\n",
            "Epoch :  78 Loss :  2.0988899001386017 Train_ACC :  0.975 Test_ACC :  0.3\n",
            "Epoch :  79 Loss :  1.1382033706177026 Train_ACC :  1.0 Test_ACC :  0.3\n",
            "Epoch :  80 Loss :  0.651089402497746 Train_ACC :  1.0 Test_ACC :  0.3\n",
            "Epoch :  81 Loss :  0.5545773910125718 Train_ACC :  1.0 Test_ACC :  0.3\n",
            "Epoch :  82 Loss :  1.030675960413646 Train_ACC :  1.0 Test_ACC :  0.3\n",
            "Epoch :  83 Loss :  0.33712497778469697 Train_ACC :  1.0 Test_ACC :  0.3\n",
            "Epoch :  84 Loss :  0.3277371500735171 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  85 Loss :  0.22046861599665135 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  86 Loss :  0.20638356485869735 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  87 Loss :  0.19183540914673358 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  88 Loss :  0.16974695213139057 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  89 Loss :  0.16231214004801586 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  90 Loss :  0.1531496933894232 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  91 Loss :  0.14614256750792265 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  92 Loss :  0.1395950330188498 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  93 Loss :  0.12900696479482576 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  94 Loss :  0.12297323544044048 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  95 Loss :  0.11327469273237512 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  96 Loss :  0.10911914601456374 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  97 Loss :  0.1054898978327401 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  98 Loss :  0.09990067750914022 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Epoch :  99 Loss :  0.09328207036014646 Train_ACC :  1.0 Test_ACC :  0.35\n",
            "Best_top3_check_point - Train_ACC :  0.95  Test_ACC :  0.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-19bFhn6mqUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from netvlad import NetVLAD\n",
        "from netvlad import EmbedNet\n",
        "import sklearn.metrics.pairwise as pairwise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp5ibA7LlL_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adapted from: https://omoindrot.github.io/triplet-loss\n",
        "\n",
        "##############\n",
        "# CAUTION !! #\n",
        "##############\n",
        "# At the moment both batch_all and batch_hard seem to pass the error testing from the source\n",
        "# mentioned above, but in practice for my experiments batch_hard converged to margin and\n",
        "# batch_all the error exploded. So for me both are not functional at the moment, but \n",
        "# _get_triplet_mask can be used to get all valid triplets and then F.triplet_margin_loss()\n",
        "# can be used to get them running. See resnet.py for how the losses are implemented in this\n",
        "# project.\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def _pairwise_distances(embeddings, squared=False):\n",
        "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
        "    Args:\n",
        "        embeddings: Variable of shape (batch_size, embed_dim)\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "    Returns:\n",
        "        pairwise_distances: Variable of shape (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the dot product between all embeddings\n",
        "    # shape (batch_size, batch_size)\n",
        "    dot_product = torch.mm(embeddings, embeddings.t())\n",
        "\n",
        "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
        "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
        "    # shape (batch_size,)\n",
        "    square_norm = torch.diag(dot_product)\n",
        "\n",
        "    # Compute the pairwise distance matrix as we have:\n",
        "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
        "    # shape (batch_size, batch_size)\n",
        "    distances = torch.unsqueeze(square_norm, 0) - 2.0 * dot_product + torch.unsqueeze(square_norm, 1)\n",
        "\n",
        "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
        "    distances = torch.clamp(distances, min=0.0)\n",
        "\n",
        "    if not squared:\n",
        "        # Not sure if needed for pytorch but does not harm\n",
        "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "        # we need to add a small epsilon where distances == 0.0\n",
        "        mask = torch.eq(distances, 0.0).float()\n",
        "        distances = distances + mask * 1e-16\n",
        "\n",
        "        distances = torch.sqrt(distances)\n",
        "\n",
        "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
        "        distances = distances * (1.0 - mask)\n",
        "\n",
        "    return distances\n",
        "\n",
        "def _get_anchor_positive_triplet_mask(labels):\n",
        "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
        "    Args:\n",
        "        labels: torch.Tensor with shape [batch_size]\n",
        "    Returns:\n",
        "        mask: Varieble with torch.ByteTensor with shape [batch_size, batch_size]\n",
        "    \"\"\"\n",
        "    # Check that i and j are distinct\n",
        "    indices_equal = torch.eye(labels.size(0)).bool()\n",
        "    if labels.is_cuda:\n",
        "        indices_equal = indices_equal.cuda()\n",
        "    indices_not_equal = ~indices_equal\n",
        "\n",
        "    # Check if labels[i] == labels[j]\n",
        "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
        "    labels_equal = torch.eq(torch.unsqueeze(labels, 0), torch.unsqueeze(labels, 1))\n",
        "\n",
        "    # Combine the two masks\n",
        "    mask = indices_not_equal & labels_equal\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_anchor_negative_triplet_mask(labels):\n",
        "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
        "    Args:\n",
        "        labels: torch.Tensor with shape [batch_size]\n",
        "    Returns:\n",
        "        mask: Variable with torch.ByteTensor with shape [batch_size, batch_size]\n",
        "    \"\"\"\n",
        "    # Check if labels[i] != labels[k]\n",
        "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
        "    labels_equal = torch.eq(torch.unsqueeze(labels, 0), torch.unsqueeze(labels, 1))\n",
        "\n",
        "    mask = ~labels_equal\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_triplet_mask(labels):\n",
        "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
        "    A triplet (i, j, k) is valid if:\n",
        "        - i, j, k are distinct\n",
        "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    Args:\n",
        "        labels: torch.Tensor with shape [batch_size]\n",
        "    \"\"\"\n",
        "    # Check that i, j and k are distinct\n",
        "    indices_equal = torch.eye(labels.size(0)).byte()\n",
        "    if labels.is_cuda:\n",
        "        indices_equal = indices_equal.cuda()\n",
        "    indices_not_equal = ~indices_equal\n",
        "    i_not_equal_j = torch.unsqueeze(indices_not_equal, 2)\n",
        "    i_not_equal_k = torch.unsqueeze(indices_not_equal, 1)\n",
        "    j_not_equal_k = torch.unsqueeze(indices_not_equal, 0)\n",
        "\n",
        "    distinct_indices = (i_not_equal_j & i_not_equal_k) & j_not_equal_k\n",
        "\n",
        "\n",
        "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    label_equal = torch.eq(torch.unsqueeze(labels, 0), torch.unsqueeze(labels, 1))\n",
        "    #if labels.is_cuda:\n",
        "    #    label_equal = label_equal.cuda()\n",
        "    i_equal_j = torch.unsqueeze(label_equal, 2)\n",
        "    i_equal_k = torch.unsqueeze(label_equal, 1)\n",
        "\n",
        "    valid_labels = i_equal_j & (~i_equal_k)\n",
        "    \n",
        "    # Combine the two masks\n",
        "    mask = distinct_indices & valid_labels.type(torch.uint8)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "    We generate all the valid triplets and average the loss over the positive ones.\n",
        "    Args:\n",
        "        labels: Variable with labels of the batch, of size (batch_size,)\n",
        "        embeddings: Variable with tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    anchor_positive_dist = torch.unsqueeze(pairwise_dist, 2)\n",
        "    anchor_negative_dist = torch.unsqueeze(pairwise_dist, 1)\n",
        "\n",
        "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
        "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
        "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
        "    # and the 2nd (batch_size, 1, batch_size)\n",
        "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
        "\n",
        "    # Put to zero the invalid triplets\n",
        "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
        "    mask = _get_triplet_mask(labels)\n",
        "    mask = Variable(mask.float())\n",
        "    triplet_loss = mask * triplet_loss\n",
        "\n",
        "    # Remove negative losses (i.e. the easy triplets)\n",
        "    triplet_loss = torch.clamp(triplet_loss, min=0.0)\n",
        "\n",
        "    # Count number of positive triplets (where triplet_loss > 0)\n",
        "    valid_triplets = torch.gt(triplet_loss, 1e-16)\n",
        "    num_positive_triplets = valid_triplets.sum().float()\n",
        "    num_valid_triplets = mask.sum()\n",
        "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
        "\n",
        "    # Get final mean triplet loss over the positive valid triplets\n",
        "    triplet_loss = triplet_loss.sum() / (num_positive_triplets + 1e-16)\n",
        "\n",
        "    return triplet_loss, fraction_positive_triplets\n",
        "\n",
        "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "    Args:\n",
        "        labels: Variable with labels of the batch, of size (batch_size,)\n",
        "        embeddings: Variable with tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    # For each anchor, get the hardest positive\n",
        "    # First, we need to get a mask for every valid positive (they should have same label)\n",
        "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
        "    mask_anchor_positive = Variable(mask_anchor_positive.float())\n",
        "\n",
        "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
        "    anchor_positive_dist = mask_anchor_positive * pairwise_dist\n",
        "\n",
        "    # shape (batch_size, 1)\n",
        "    hardest_positive_dist, _ = torch.max(anchor_positive_dist, dim=1, keepdim=True)\n",
        "\n",
        "    # For each anchor, get the hardest negative\n",
        "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
        "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
        "    mask_anchor_negative = Variable(mask_anchor_negative.float())\n",
        "\n",
        "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
        "    max_anchor_negative_dist, _ = torch.max(pairwise_dist, dim=1, keepdim=True)\n",
        "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
        "\n",
        "    # shape (batch_size,)\n",
        "    hardest_negative_dist, _ = torch.min(anchor_negative_dist, dim=1, keepdim=True)\n",
        "\n",
        "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
        "    triplet_loss = torch.clamp(hardest_positive_dist - hardest_negative_dist + margin, min=0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = triplet_loss.mean()\n",
        "\n",
        "    return triplet_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTGPXmXWHv3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def largest_indices(array: np.ndarray, n: int) -> tuple:\n",
        "    \"\"\"Returns the n largest indices from a numpy array.\n",
        "    Arguments:\n",
        "        array {np.ndarray} -- data array\n",
        "        n {int} -- number of elements to select\n",
        "    Returns:\n",
        "        tuple[np.ndarray, np.ndarray] -- tuple of ndarray\n",
        "        each ndarray is index\n",
        "    \"\"\"\n",
        "    flat = array.flatten()\n",
        "    indices = np.argpartition(flat, -n)[-n:]\n",
        "    indices = indices[np.argsort(-flat[indices])]\n",
        "    return ((indices+1)/2+0.5).astype('int64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISamep9VycqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_VLAD = TensorDataset(x_data.permute(1,0,2), y_data.type(torch.LongTensor))\n",
        "loader_VLAD = DataLoader(dataset_VLAD, batch_size=10, shuffle=True)\n",
        "\n",
        "dataset_VLAD_test = TensorDataset(x_test_data.permute(1,0,2), y_test_data.type(torch.LongTensor))\n",
        "loader_VLAD_test = DataLoader(dataset_VLAD_test, batch_size=10, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auA6dW_Ey2PG",
        "colab_type": "code",
        "outputId": "e061c916-f19d-438b-ef6c-a6a9e22bb265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "back_bone_model = LSTM(8, 32, batch_size=10, output_dim=class_cnt, num_layers=2)\n",
        "back_bone_model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/chekcpoint_20200506_best_top3.pth'), strict=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Zj3LLVKP5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model for embedding\n",
        "net_vlad = NetVLAD(num_clusters=20, dim=64, alpha=1.0)\n",
        "model = EmbedNet(back_bone_model, net_vlad).cuda()\n",
        "# criterion = HardTripletLoss(margin=0.1).cuda()\n",
        "# criterion = batch_all_triplet_loss(margin=0.1).cuda()\n",
        "optimiser_netvlad = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvfJ5dWrQtLa",
        "colab_type": "code",
        "outputId": "06b927fa-0c5e-4e03-ce74-122985586752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for t in range(3000):\n",
        "\n",
        "  correct = 0\n",
        "  \n",
        "  for xx_vlad, yy_vlad in loader_VLAD:\n",
        "    xx_vlad = xx_vlad.permute(1,0,2)\n",
        "    output = model(xx_vlad.cuda())\n",
        "    triplet_loss = batch_all_triplet_loss(yy_vlad.cuda(),output,margin=0.1)\n",
        "    # Zero out gradient, else they will accumulate between epochs\n",
        "    optimiser_netvlad.zero_grad()\n",
        "\n",
        "    # Backward pass\n",
        "    triplet_loss[0].backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimiser_netvlad.step()\n",
        "\n",
        "  for xx_vlad_test, yy_vlad_test in loader_VLAD_test:\n",
        "    xx_vlad_test = xx_vlad_test.permute(1,0,2)\n",
        "    vald_out_test = model(xx_vlad_test.cuda())\n",
        "\n",
        "  similarity = pairwise.cosine_similarity(X=vald_out_test.cpu().detach().numpy(), Y=vald_out_train.cpu().detach().numpy(), dense_output=True)\n",
        "  \n",
        "  for i in range(len(similarity)) :\n",
        "    if yy_vlad_test[i].item() in largest_indices(similarity[i],3) : \n",
        "      correct += 1\n",
        "  \n",
        "  acc_val = correct/10\n",
        "\n",
        "  if t % 10 == 0:\n",
        "    print(\"Epoch \", t, \"Triplet_loss: \", triplet_loss[0].item(), \"Test_ACC :\", acc_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 Triplet_loss:  0.11106157302856445 Test_ACC : 0.1\n",
            "Epoch  10 Triplet_loss:  0.10560329258441925 Test_ACC : 0.2\n",
            "Epoch  20 Triplet_loss:  0.07312006503343582 Test_ACC : 0.1\n",
            "Epoch  30 Triplet_loss:  0.09156176447868347 Test_ACC : 0.0\n",
            "Epoch  40 Triplet_loss:  0.10847645998001099 Test_ACC : 0.1\n",
            "Epoch  50 Triplet_loss:  0.09567942470312119 Test_ACC : 0.1\n",
            "Epoch  60 Triplet_loss:  0.0 Test_ACC : 0.1\n",
            "Epoch  70 Triplet_loss:  0.0673879086971283 Test_ACC : 0.0\n",
            "Epoch  80 Triplet_loss:  0.0 Test_ACC : 0.0\n",
            "Epoch  90 Triplet_loss:  0.023257054388523102 Test_ACC : 0.0\n",
            "Epoch  100 Triplet_loss:  0.0 Test_ACC : 0.1\n",
            "Epoch  110 Triplet_loss:  0.0018243566155433655 Test_ACC : 0.1\n",
            "Epoch  120 Triplet_loss:  0.0 Test_ACC : 0.1\n",
            "Epoch  130 Triplet_loss:  0.0 Test_ACC : 0.1\n",
            "Epoch  140 Triplet_loss:  0.10040651261806488 Test_ACC : 0.2\n",
            "Epoch  150 Triplet_loss:  0.0528729110956192 Test_ACC : 0.2\n",
            "Epoch  160 Triplet_loss:  0.0 Test_ACC : 0.1\n",
            "Epoch  170 Triplet_loss:  0.10126084834337234 Test_ACC : 0.0\n",
            "Epoch  180 Triplet_loss:  0.019181467592716217 Test_ACC : 0.1\n",
            "Epoch  190 Triplet_loss:  0.09726563841104507 Test_ACC : 0.1\n",
            "Epoch  200 Triplet_loss:  0.0973806381225586 Test_ACC : 0.0\n",
            "Epoch  210 Triplet_loss:  0.07941916584968567 Test_ACC : 0.1\n",
            "Epoch  220 Triplet_loss:  0.06170300766825676 Test_ACC : 0.1\n",
            "Epoch  230 Triplet_loss:  0.09008858352899551 Test_ACC : 0.2\n",
            "Epoch  240 Triplet_loss:  0.021595485508441925 Test_ACC : 0.1\n",
            "Epoch  250 Triplet_loss:  0.09054761379957199 Test_ACC : 0.1\n",
            "Epoch  260 Triplet_loss:  0.1048446074128151 Test_ACC : 0.2\n",
            "Epoch  270 Triplet_loss:  0.09940768033266068 Test_ACC : 0.1\n",
            "Epoch  280 Triplet_loss:  0.06704098731279373 Test_ACC : 0.2\n",
            "Epoch  290 Triplet_loss:  0.0 Test_ACC : 0.2\n",
            "Epoch  300 Triplet_loss:  0.07153349369764328 Test_ACC : 0.1\n",
            "Epoch  310 Triplet_loss:  0.0 Test_ACC : 0.2\n",
            "Epoch  320 Triplet_loss:  0.0 Test_ACC : 0.1\n",
            "Epoch  330 Triplet_loss:  0.10715415328741074 Test_ACC : 0.1\n",
            "Epoch  340 Triplet_loss:  0.0 Test_ACC : 0.0\n",
            "Epoch  350 Triplet_loss:  0.05007551982998848 Test_ACC : 0.1\n",
            "Epoch  360 Triplet_loss:  0.13970695436000824 Test_ACC : 0.1\n",
            "Epoch  370 Triplet_loss:  0.07334993779659271 Test_ACC : 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-abfb8f5668ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moptimiser_netvlad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mxx_vlad_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy_vlad_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_VLAD_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJhgtGYRfqfg",
        "colab_type": "code",
        "outputId": "820a9039-7cda-4fd1-ac30-abdbb56d2604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vald_out_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 1280])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2_cLA8C5Q58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(),'/content/drive/My Drive/Colab Notebooks/VLAD_Checkpoint_20200506.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFqAunyR6ORE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_VLAD_test = TensorDataset(x_test_data.permute(1,0,2), y_test_data.type(torch.LongTensor))\n",
        "loader_VLAD_test = DataLoader(dataset_VLAD_test, batch_size=10, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3h4hQl05GrK",
        "colab_type": "code",
        "outputId": "faf7625a-285e-4903-b55e-e40f719997aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "back_bone_model = LSTM(8, 32, batch_size=10, output_dim=class_cnt, num_layers=2)\n",
        "net_vlad = NetVLAD(num_clusters=20, dim=64, alpha=1.0)\n",
        "model = EmbedNet(back_bone_model, net_vlad).cuda()\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/VLAD_Checkpoint_20200506.pth'), strict=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUrbxuziEaMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "for xx_vlad_test, yy_vlad_test in loader_VLAD_test:\n",
        "  xx_vlad_test = xx_vlad_test.permute(1,0,2)\n",
        "  vald_out_test = model(xx_vlad_test.cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8vbyxS55Dut",
        "colab_type": "code",
        "outputId": "64dd25ce-abd6-4e14-d9ea-ff6ac2edabd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vald_out_test.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1280])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNoXtmlM9FUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity = pairwise.cosine_similarity(X=vald_out_test.cpu().detach().numpy(), Y=vald_out_train.cpu().detach().numpy(), dense_output=True)\n",
        "#similarity = pairwise.cosine_similarity(X=vald_out_train.cpu().detach().numpy(), Y=vald_out_train.cpu().detach().numpy(), dense_output=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUaXUQYZADN2",
        "colab_type": "code",
        "outputId": "dc24e459-93e5-4ae5-b33c-9054579ed4b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "similarity"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5478743 , 0.53230685, 0.4983186 , 0.48963085, 0.8509887 ,\n",
              "        0.6484924 , 0.6814134 , 0.45905724, 0.4808666 , 0.5824408 ,\n",
              "        0.5939707 , 0.31204283, 0.5617833 , 0.45794606, 0.4652688 ,\n",
              "        0.59405875, 0.601035  , 0.5306729 , 0.44606566, 0.49725428,\n",
              "        0.68929625, 0.49754393, 0.39432213, 0.5127539 , 0.39100105,\n",
              "        0.6662994 , 0.6197829 , 0.36044106, 0.5415908 , 0.5732816 ,\n",
              "        0.3393296 , 0.6062944 , 0.3307428 , 0.5208565 , 0.59452766,\n",
              "        0.6707636 , 0.69013876, 0.56984925, 0.5065799 , 0.7351494 ],\n",
              "       [0.69541454, 0.760062  , 0.5185545 , 0.697226  , 0.45075762,\n",
              "        0.47041717, 0.47507495, 0.5299672 , 0.93823665, 0.44241655,\n",
              "        0.58940727, 0.5294286 , 0.65996844, 0.42700607, 0.56729025,\n",
              "        0.66303515, 0.40691844, 0.4702618 , 0.6711115 , 0.90476036,\n",
              "        0.45361468, 0.4212109 , 0.61699426, 0.5325421 , 0.73800075,\n",
              "        0.50529695, 0.77864844, 0.69477594, 0.44714504, 0.6533525 ,\n",
              "        0.4814344 , 0.4333452 , 0.53020203, 0.44670537, 0.567487  ,\n",
              "        0.44420516, 0.47937357, 0.49912927, 0.6067013 , 0.391818  ],\n",
              "       [0.571736  , 0.5922601 , 0.60893655, 0.5489091 , 0.3278428 ,\n",
              "        0.46497354, 0.37305638, 0.659932  , 0.5063356 , 0.44813827,\n",
              "        0.47929892, 0.93106437, 0.46396962, 0.5045548 , 0.56407905,\n",
              "        0.4438186 , 0.42183045, 0.70014584, 0.39122167, 0.49361795,\n",
              "        0.5032827 , 0.50669205, 0.43605453, 0.45661896, 0.78773755,\n",
              "        0.38811386, 0.43145478, 0.76205933, 0.43412948, 0.50721294,\n",
              "        0.6525324 , 0.384226  , 0.86422133, 0.39540875, 0.46420324,\n",
              "        0.5942625 , 0.39142329, 0.55637044, 0.6612135 , 0.478543  ],\n",
              "       [0.7092927 , 0.69288254, 0.68253005, 0.58644813, 0.49174815,\n",
              "        0.5361112 , 0.48908907, 0.5676128 , 0.53355247, 0.35703495,\n",
              "        0.5803144 , 0.74852586, 0.49233925, 0.48055804, 0.4506566 ,\n",
              "        0.49503553, 0.35903308, 0.6725864 , 0.39710885, 0.54169965,\n",
              "        0.46479106, 0.36184523, 0.42469823, 0.40569085, 0.7504179 ,\n",
              "        0.49651048, 0.503128  , 0.8023293 , 0.38613296, 0.5508354 ,\n",
              "        0.52897245, 0.3210854 , 0.74010503, 0.28492114, 0.56902903,\n",
              "        0.58040583, 0.43075854, 0.54112345, 0.5665097 , 0.55888355],\n",
              "       [0.42572072, 0.49378303, 0.68860275, 0.5431509 , 0.5910622 ,\n",
              "        0.709576  , 0.41778392, 0.74538225, 0.44267887, 0.5859134 ,\n",
              "        0.6487098 , 0.43391448, 0.6686519 , 0.5200072 , 0.576852  ,\n",
              "        0.6084176 , 0.51686597, 0.71907836, 0.5178628 , 0.44225702,\n",
              "        0.5640291 , 0.4781239 , 0.5176683 , 0.758367  , 0.4566543 ,\n",
              "        0.36759055, 0.577398  , 0.4567654 , 0.4516843 , 0.46084762,\n",
              "        0.701386  , 0.5024315 , 0.43624052, 0.57532454, 0.6805964 ,\n",
              "        0.62066716, 0.7836604 , 0.5699865 , 0.53528976, 0.5796811 ],\n",
              "       [0.486446  , 0.58543086, 0.5075401 , 0.5976556 , 0.40037388,\n",
              "        0.6565402 , 0.33153164, 0.71115774, 0.5158736 , 0.7601131 ,\n",
              "        0.64258355, 0.5680717 , 0.6830307 , 0.28345704, 0.9837656 ,\n",
              "        0.5497725 , 0.7257139 , 0.6629418 , 0.4499467 , 0.508634  ,\n",
              "        0.6144614 , 0.5142164 , 0.4276736 , 0.552974  , 0.5072311 ,\n",
              "        0.3371635 , 0.62537664, 0.49548414, 0.46346807, 0.45384622,\n",
              "        0.51744795, 0.37497103, 0.69334096, 0.39640942, 0.6029564 ,\n",
              "        0.6267386 , 0.7123515 , 0.42424548, 0.8492262 , 0.39675924],\n",
              "       [0.45654523, 0.529171  , 0.700899  , 0.48802778, 0.52024114,\n",
              "        0.80900985, 0.38574645, 0.6971147 , 0.47666484, 0.64743423,\n",
              "        0.56875163, 0.4323624 , 0.71337414, 0.43185052, 0.7045556 ,\n",
              "        0.62577045, 0.5872152 , 0.7075862 , 0.4915404 , 0.4717173 ,\n",
              "        0.6054542 , 0.43025345, 0.436329  , 0.6407563 , 0.3733582 ,\n",
              "        0.31007105, 0.66368437, 0.4370094 , 0.42358223, 0.42368588,\n",
              "        0.5730826 , 0.4288561 , 0.51936865, 0.52916116, 0.72522694,\n",
              "        0.61187094, 0.87782395, 0.5640491 , 0.65983903, 0.5177634 ],\n",
              "       [0.34537238, 0.47082448, 0.42683983, 0.5908911 , 0.47209528,\n",
              "        0.5050107 , 0.48319685, 0.42857003, 0.67242926, 0.4775805 ,\n",
              "        0.49021363, 0.3707607 , 0.6291109 , 0.55460304, 0.426087  ,\n",
              "        0.6038123 , 0.414856  , 0.37477848, 0.9069986 , 0.640799  ,\n",
              "        0.50967723, 0.5974426 , 0.85759884, 0.7373335 , 0.4028479 ,\n",
              "        0.5161014 , 0.56368196, 0.40215695, 0.554003  , 0.52211946,\n",
              "        0.6178358 , 0.7555239 , 0.28513908, 0.803461  , 0.4965347 ,\n",
              "        0.37908703, 0.5795371 , 0.5405163 , 0.40396535, 0.46500403],\n",
              "       [0.58443445, 0.511931  , 0.55454016, 0.40016472, 0.52347714,\n",
              "        0.47621283, 0.756781  , 0.35313353, 0.5054194 , 0.36691839,\n",
              "        0.34324503, 0.37128887, 0.4877569 , 0.6942688 , 0.42037243,\n",
              "        0.5651517 , 0.43958557, 0.3832153 , 0.5381163 , 0.52614385,\n",
              "        0.5812698 , 0.34358236, 0.50041217, 0.5312036 , 0.3967166 ,\n",
              "        0.7636645 , 0.5645186 , 0.39072725, 0.38601613, 0.43187383,\n",
              "        0.41870803, 0.60657585, 0.32674646, 0.61549896, 0.5997063 ,\n",
              "        0.5253198 , 0.657436  , 0.5849552 , 0.503768  , 0.5233081 ],\n",
              "       [0.6935483 , 0.8078806 , 0.5275251 , 0.6215644 , 0.46034   ,\n",
              "        0.53693295, 0.48276716, 0.53319806, 0.8416484 , 0.46920735,\n",
              "        0.53916633, 0.6108243 , 0.68242186, 0.4586896 , 0.6357522 ,\n",
              "        0.5976496 , 0.44847724, 0.49345925, 0.6388521 , 0.8361218 ,\n",
              "        0.5984524 , 0.38332438, 0.57507575, 0.47138703, 0.68108404,\n",
              "        0.50315595, 0.7648411 , 0.681548  , 0.39034203, 0.56678873,\n",
              "        0.42475632, 0.4584878 , 0.6206636 , 0.48721617, 0.56695896,\n",
              "        0.608511  , 0.5808419 , 0.57940066, 0.76133454, 0.4557861 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d2X64R-FqSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def largest_indices(array: np.ndarray, n: int) -> tuple:\n",
        "    \"\"\"Returns the n largest indices from a numpy array.\n",
        "    Arguments:\n",
        "        array {np.ndarray} -- data array\n",
        "        n {int} -- number of elements to select\n",
        "    Returns:\n",
        "        tuple[np.ndarray, np.ndarray] -- tuple of ndarray\n",
        "        each ndarray is index\n",
        "    \"\"\"\n",
        "    flat = array.flatten()\n",
        "    indices = np.argpartition(flat, -n)[-n:]\n",
        "    indices = indices[np.argsort(-flat[indices])]\n",
        "    return ((indices+1)/2+0.5).astype('int64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rmlVX-X9epN",
        "colab_type": "code",
        "outputId": "d64738c2-ec34-4c6e-ed34-4c355959494b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "for i in range(len(similarity)) :\n",
        "  print(largest_indices(similarity[i],3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3 20 19]\n",
            "[ 5 10 14]\n",
            "[ 6 17 13]\n",
            "[14 13  6]\n",
            "[19 12  4]\n",
            "[ 8 20  5]\n",
            "[19  3 18]\n",
            "[10 12 17]\n",
            "[13  4  7]\n",
            "[ 5 10  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbhK72sHA8x",
        "colab_type": "code",
        "outputId": "dba9d267-2bda-400b-813f-54cfb3daae60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "yy_vlad_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5,  1, 16, 15, 14, 12, 10,  2,  7, 13])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAJ6o8HDEQAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}